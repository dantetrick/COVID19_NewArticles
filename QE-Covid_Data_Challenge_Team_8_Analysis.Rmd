---
title: "What makes a COVID-19 news article more appealing to readers?"
output:
  html_document:
    df_print: paged
---

## Analysis Question:

Using the below data, identify the news articles that generate the most/least web traffic per site. Once the two groups of news articles have been identified, pick a couple sources, read some articles for content, then develop QE codes that can be used to identify why certain articles related to COVID are more appealing to readers than others. 

### Data Used for Analysis

DATASET3: ENGLISH NEWS ARTICLES THAT MENTION "CORONA VIRUS" OR "CORONAVIRUS" OR "COVID" (BY WEBHOSE.IO)
Link: https://webhose.io/free-datasets/news-articles-that-mention-corona-virus/
Format: JSON | Size: 13.7GB | Crawled: Dec, 2019 - Mar, 2020
Access: Free, but you have to create a profile on webhose.io
Main variables: Social media shares and likes; Site name; Site section; Section title; Country; Entities; Participants count; Replies count; Spam score; Performance score; Text; External links

#### Set Project Options, Load Packages & Functions

Options are preference only - not required. Removing scientific notation and setting time date seconds to 3 digits
```{r}
options(scipen = 999)
options(digits.secs = 3)           
gc()
```

#### Load Packages

Load required packages to conduct analysis. User will need to download any packages that aren't in list.
```{r, warning=F, message=F}
pkgs <- sapply(c("jsonlite", "lubridate" ,"tidyverse", "data.table", "ndjson")
               , function(x) {require(x, character.only = T)})
```

#### Create a list of the columns that are used in analysis. 

Initially an analyisis was conducted on a full data set of 794 columns. Found 44 columns with at least 50% non N/A or blank values. Kept 29 of the columns after removing duplicated columnar values.   
```{r}
ColumnsToKeep <- 
  c("author","crawled","language","ord_in_thread","published","text"
    ,"thread.country","thread.main_image","thread.participants_count"
    ,"thread.performance_score","thread.published","thread.replies_count"
    ,"thread.section_title","thread.site","thread.site_full" ,"thread.site_section"
    ,"thread.site_type" ,"thread.social.facebook.comments","thread.social.facebook.likes" 
    ,"thread.social.facebook.shares","thread.social.gplus.shares" ,"thread.social.linkedin.shares"
    ,"thread.social.pinterest.shares" ,"thread.social.stumbledupon.shares"
    ,"thread.social.vk.shares" ,"thread.title","title","url","thread.url")

```

#### List Data Files

After unzipping file data set you'll find 31 files. Set path to location of unzipped files and this function lists them in preparation for loading into single list format. 
```{r}
ls_data <- list.files("E:/COVID19/data", "json", full.names = T)
```

#### Load all JSON data using stream_in function

Creates a data frame and retains columns identified by ColumnsToKeep listed above. Also note the filter, adjust the filter to the news agency of your choice. 

NOTE: Takes about 30 mins to load all data

```{r, warning=F}
dt_datastreams <- lapply(ls_data, function(x){ data.frame(stream_in(x)[str_detect(thread.site, "foxnews.com|cnn.com"),])[, ColumnsToKeep]}) 
```

#### Row bind all dataframes into single table

After binding rows, format date times and remove all non-english characters from thread title and text. Use the thread URL to aggregate measurements of important variables to define news traffic across all threads per URL. The 0 value in ord_in_thread is the original news article, filter data to assure we are using original article content and not reader content after measurements were taken. Calculate time differences between min_crawldate and thread.published dates to give a sense of how long the article was posted to allow traffic info. For each URL, create a measurement of total thread interactions by summing each thread social numeric variables that will give a sense of use. Measure the thread participation per day by dividing the total interaction by the time between it was posted and crawled. Create a dense ranking of which threads have the most interactions per day by thread site. Last, create a dense ranking of each site size to allow users to judge big site from small. 

```{r, warning=F, message=F}
dt_datastreams <- data.table(rbindlist(dt_datastreams))[
  ,`:=`(crawled = strptime(str_sub(crawled,1, 23), "%Y-%m-%dT%H:%M:%OS")
        , published = strptime(str_sub(published,1, 23), "%Y-%m-%dT%H:%M:%OS")
        , thread.published = strptime(str_sub(thread.published,1, 23), "%Y-%m-%dT%H:%M:%OS")
        , thread.title = iconv(thread.title, from = 'UTF-8', to = 'ASCII//TRANSLIT')
        , text = iconv(text, from = 'UTF-8', to = 'ASCII//TRANSLIT')
        , thread.section_title = iconv(thread.section_title, from = 'UTF-8', to = 'ASCII//TRANSLIT')
        )][
          !is.na(text),][
          , `:=`(total_threads = .N
                 , thread.participants_count = max(thread.participants_count)
                 , thread.replies_count = max(thread.replies_count)
                 , thread.social.facebook.comments = max(thread.social.facebook.comments)
                 , thread.social.facebook.likes = max(thread.social.facebook.likes)
                 , thread.social.facebook.shares = max(thread.social.facebook.shares)
                 , thread.social.gplus.shares = max(thread.social.gplus.shares)
                 , thread.social.linkedin.shares = max(thread.social.linkedin.shares)
                 , thread.social.pinterest.shares = max(thread.social.pinterest.shares)
                 , thread.social.stumbledupon.shares = max(thread.social.stumbledupon.shares)
                 , thread.social.vk.shares = max(thread.social.vk.shares)
                 , max_crawldate = max(crawled)
                 , min_threadpublisheddate = min(thread.published)
                 ), by = "thread.url"
          ][
            ord_in_thread==0,][
            , `:=`(crawled_threadpublished_daysdiff = round(as.numeric(difftime(max_crawldate, min_threadpublisheddate, units = "days")),2)
          )][
            , `:=`(thread_total_interactions = sum(thread.participants_count, thread.replies_count, thread.social.facebook.comments, thread.social.facebook.likes, thread.social.facebook.shares,thread.social.gplus.shares, thread.social.linkedin.shares, thread.social.pinterest.shares, thread.social.stumbledupon.shares, thread.social.vk.shares) )
            , by = "thread.url"
          ][
            , `:=`(thread_participants_per_day = round(thread_total_interactions/crawled_threadpublished_daysdiff,2)
                   )][
            , `:=`(thread_ranked_per_site = dense_rank(thread_participants_per_day)
                   , site_size = .N)
            , by = "thread.site"
          ][ , `:=`(site_size_rank = dense_rank(site_size)
                      )]

```


#### Write out data
```{r, warning=F, message=F}

fwrite(dt_datastreams, "E:/COVID19/ENGLISH NEWS ARTICLES THAT MENTION CORONA VIRUS FOX AND CNN.csv")

```


#### Take 20 row sample to show
```{r, warning=F, message=F}
set.seed(8675309)
dt_datastreams[sample(20), ]

```
